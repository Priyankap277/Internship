{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ec3011d",
   "metadata": {},
   "source": [
    "\n",
    "Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10\n",
    "jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.shine.com/\n",
    "2. Enter “Data Analyst” in “Job title, Skills” field and enter “Bangalore” in “enter the location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37e49e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd \n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8d88cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        Title  \\\n",
      "0                           Lead Data Analyst   \n",
      "1                                Data Analyst   \n",
      "2                    Vacancy For Data Analyst   \n",
      "3                       Clinical Data Analyst   \n",
      "4                           Data Modeler data   \n",
      "5                               Data Modeller   \n",
      "6                      Data Modeler Bangalore   \n",
      "7                                Data Modeler   \n",
      "8                       Clinical Data Analyst   \n",
      "9  Data Analyst Hiring Fresher and Experience   \n",
      "\n",
      "                                 Employer  Expriennce  \n",
      "0           ara resources private limited  4 to 9 Yrs  \n",
      "1       diraa hr services hiring for mncs   0 to 1 Yr  \n",
      "2                yogita staffing solution  0 to 3 Yrs  \n",
      "3                           techno endura   0 to 1 Yr  \n",
      "4  boyen haddin consulting and technol...  3 to 6 Yrs  \n",
      "5  boyen haddin consulting and technol...  3 to 6 Yrs  \n",
      "6  boyen haddin consulting and technol...  3 to 6 Yrs  \n",
      "7  boyen haddin consulting and technol...  3 to 6 Yrs  \n",
      "8                         quiscon biotech  0 to 2 Yrs  \n",
      "9                       kavya interprises  0 to 4 Yrs  \n"
     ]
    }
   ],
   "source": [
    "driver= webdriver.Chrome()\n",
    "driver.get(\"https://www.shine.com/\")\n",
    "designation=driver.find_element(By.XPATH,\"//*[@id=\\\"webSearchBar\\\"]/input\")\n",
    "designation.click()\n",
    "driver.implicitly_wait(2)\n",
    "designation=driver.find_element(By.ID,\"id_q\")\n",
    "designation.send_keys('Data Analyst')\n",
    "designation=driver.find_element(By.ID,\"id_loc\")\n",
    "designation.send_keys('Bangalore')\n",
    "designation=driver.find_element(By.XPATH,\"//*[@id=\\\"__next\\\"]/div[4]/div/div[2]/div[2]/div/form/div/div[2]/div/button\")\n",
    "designation.click()\n",
    "driver.implicitly_wait(5)\n",
    "titles = []\n",
    "employers = []\n",
    "exprience = []\n",
    "title_tags=driver.find_elements(By.XPATH,\"//div/h2/a\")\n",
    "for i in title_tags:\n",
    "    titles.append(i.text)\n",
    "#jobCard_jobCard_cName__mYnow   \n",
    "\n",
    "employer_tags=driver.find_elements(By.XPATH,'//div[@class=\"jobCard_jobCard_cName__mYnow\"]/span')\n",
    "for i in employer_tags:\n",
    "    employers.append(i.text)\n",
    "\n",
    "    # jobCard_jobCard_lists_item__YxRkV jobCard_jobIcon__3FB1t\n",
    "#tml body div#__next div.main div.container.jsrpcomponent_mobile_container__boS9b.jsrpFixBody div.jsrpcomponent_jsrp__DUsWy.mt-15.jsrp div.jsrpcomponent_jsrp_leftPanel__2nm3w.jsrp_leftPanel div.leftpanel_jsrp_leftPanel_container__AVUCY.custom_scroll div.leftpanel_jsrp_leftPanel_wrap__YXu_c.jsrp_leftPanel_wrap div#1.parentClass.position-relative div div.jobCard_jobCard__jjUmu.active.white-box-border.jobCard div div.jobCard_jobCard_lists__fdnsc div.jobCard_jobCard_lists_item__YxRkV.jobCard_jobIcon__3FB1t    \n",
    "exprienc_tags = driver.find_elements(By.CLASS_NAME, 'jobCard_jobIcon__3FB1t')\n",
    "for i in exprienc_tags:\n",
    "    exprience.append(i.text)\n",
    "    \n",
    "data = {'Title': titles[:10], 'Employer': employers[:10], 'Expriennce': exprience[:10]}\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d2a38a",
   "metadata": {},
   "source": [
    "Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.shine.com/\n",
    "2. Enter “Data Scientist” in “Job title, Skills” field and enter “Bangalore” in “enter thelocation” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b266b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title  \\\n",
      "0                                     Data Scientist   \n",
      "1                                     Data Scientist   \n",
      "2      Lead Data Scientist/ Principal Data Scientist   \n",
      "3                              Senior Data Scientist   \n",
      "4  Vacancy For Data Scientist Fresher and Experience   \n",
      "5                                Lead Data Scientist   \n",
      "6                                     Data Scientist   \n",
      "7                      Data Scientist Urgent Vacancy   \n",
      "8                                     Data Scientist   \n",
      "9                              Pharma Data Scientist   \n",
      "\n",
      "                        Employer   Expriennce  \n",
      "0  acme services private limited   3 to 5 Yrs  \n",
      "1            ltimindtree limited   6 to 8 Yrs  \n",
      "2                        fractal   5 to 9 Yrs  \n",
      "3                       neostats  7 to 11 Yrs  \n",
      "4       yogita staffing solution   0 to 3 Yrs  \n",
      "5                          aereo  7 to 10 Yrs  \n",
      "6      people staffing solutions   4 to 9 Yrs  \n",
      "7       yogita staffing solution   0 to 3 Yrs  \n",
      "8              diraa hr services   0 to 4 Yrs  \n",
      "9                quiscon biotech    0 to 1 Yr  \n"
     ]
    }
   ],
   "source": [
    "driver= webdriver.Chrome()\n",
    "driver.get(\"https://www.shine.com/\")\n",
    "designation=driver.find_element(By.XPATH,\"//*[@id=\\\"webSearchBar\\\"]/input\")\n",
    "designation.click()\n",
    "driver.implicitly_wait(2)\n",
    "designation=driver.find_element(By.ID,\"id_q\")\n",
    "designation.send_keys('Data Scientist')\n",
    "designation=driver.find_element(By.ID,\"id_loc\")\n",
    "designation.send_keys('Bangalore')\n",
    "designation=driver.find_element(By.XPATH,\"//*[@id=\\\"__next\\\"]/div[4]/div/div[2]/div[2]/div/form/div/div[2]/div/button\")\n",
    "designation.click()\n",
    "driver.implicitly_wait(5)\n",
    "titles = []\n",
    "employers = []\n",
    "exprience = []\n",
    "title_tags=driver.find_elements(By.XPATH,\"//div/h2/a\")\n",
    "for i in title_tags:\n",
    "    titles.append(i.text)\n",
    "#jobCard_jobCard_cName__mYnow   \n",
    "\n",
    "employer_tags=driver.find_elements(By.XPATH,'//div[@class=\"jobCard_jobCard_cName__mYnow\"]/span')\n",
    "for i in employer_tags:\n",
    "    employers.append(i.text)\n",
    "\n",
    "    # jobCard_jobCard_lists_item__YxRkV jobCard_jobIcon__3FB1t\n",
    "#tml body div#__next div.main div.container.jsrpcomponent_mobile_container__boS9b.jsrpFixBody div.jsrpcomponent_jsrp__DUsWy.mt-15.jsrp div.jsrpcomponent_jsrp_leftPanel__2nm3w.jsrp_leftPanel div.leftpanel_jsrp_leftPanel_container__AVUCY.custom_scroll div.leftpanel_jsrp_leftPanel_wrap__YXu_c.jsrp_leftPanel_wrap div#1.parentClass.position-relative div div.jobCard_jobCard__jjUmu.active.white-box-border.jobCard div div.jobCard_jobCard_lists__fdnsc div.jobCard_jobCard_lists_item__YxRkV.jobCard_jobIcon__3FB1t    \n",
    "exprienc_tags = driver.find_elements(By.CLASS_NAME, 'jobCard_jobIcon__3FB1t')\n",
    "for i in exprienc_tags:\n",
    "    exprience.append(i.text)\n",
    "    \n",
    "data = {'Title': titles[:10], 'Employer': employers[:10], 'Expriennce': exprience[:10]}\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e832e25",
   "metadata": {},
   "source": [
    "Q3: In this question you have to scrape data using the filters available on the webpage\n",
    "You have to use the location and salary filter.\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "You have to scrape the job-title, job-location, company name, experience required.\n",
    "The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "bf379bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Title  \\\n",
      "0                 Data Scientist/Data Analyst-17740   \n",
      "1  Data Scientist /Quantitative Equity Researcher 2   \n",
      "2                      Data Scientist (Theoretical)   \n",
      "3                             Senior Data Scientist   \n",
      "4                              Staff Data Scientist   \n",
      "5                       Data Scientist- BLR HYD GGN   \n",
      "6                               Lead Data Scientist   \n",
      "7                                    Data Scientist   \n",
      "8                                    Data Scientist   \n",
      "9                       Data Scientist - SQL/Python   \n",
      "\n",
      "                                 Employer    Expriennce  \n",
      "0                 gemraj technologies ltd    3 to 5 Yrs  \n",
      "1                  pinebridge investments     0 to 1 Yr  \n",
      "2                    torcai digital media     0 to 1 Yr  \n",
      "3                         infovision inc.     0 to 1 Yr  \n",
      "4                               legalzoom     0 to 1 Yr  \n",
      "5                                 genpact     0 to 1 Yr  \n",
      "6                             three steps     0 to 1 Yr  \n",
      "7  flipped.ai - transforming talent ac...  10 to 20 Yrs  \n",
      "8                          osource global    2 to 7 Yrs  \n",
      "9                          arting digital     0 to 1 Yr  \n"
     ]
    }
   ],
   "source": [
    "#.jobCard div h2 a  /html/body/div[1]/div[1]/div[4]/div/div[2]/div[1]/div/div/div[9]/div[1]/div[1]/h2/a\n",
    "driver= webdriver.Chrome()\n",
    "driver.get(\"https://www.shine.com/\")\n",
    "designation=driver.find_element(By.XPATH,\"//*[@id=\\\"webSearchBar\\\"]/input\")\n",
    "designation.click()\n",
    "driver.implicitly_wait(2)\n",
    "designation=driver.find_element(By.ID,\"id_q\")\n",
    "designation.send_keys('Data Scientist')\n",
    "designation=driver.find_element(By.XPATH,\"//*[@id=\\\"__next\\\"]/div[4]/div/div[2]/div[2]/div/form/div/div[2]/div/button\")\n",
    "designation.click()\n",
    "button = driver.find_element(By.XPATH, \"//button[contains( text( ), 'Salary')]\")\n",
    "button.click()\n",
    "label = driver.find_element(By.XPATH, \"//label[contains( text( ), '3 To 5 Lakh')]\")\n",
    "label.click()\n",
    "locationLink = driver.find_element(By.XPATH, \"//li[contains( text( ), 'Location')]\")\n",
    "locationLink.click()\n",
    "locationFilter = driver.find_element(By.ID, \"search\")\n",
    "locationFilter.send_keys('Delhi')\n",
    "locationLink = driver.find_element(By.XPATH, '//label[@for=\"filter_jFLoc_406\"]')\n",
    "locationLink.click()\n",
    "button = driver.find_element(By.XPATH, \"//button[contains( text( ), 'Show Results')]\")\n",
    "button.click()\n",
    "driver.implicitly_wait(5)\n",
    "titles = []\n",
    "employers = []\n",
    "exprience = []\n",
    "title_tags=driver.find_elements(By.XPATH,\"//div/h2/a\")\n",
    "for i in title_tags:\n",
    "    titles.append(i.text)\n",
    "#jobCard_jobCard_cName__mYnow   \n",
    "\n",
    "employer_tags=driver.find_elements(By.XPATH,'//div[@class=\"jobCard_jobCard_cName__mYnow\"]/span')\n",
    "for i in employer_tags:\n",
    "    employers.append(i.text)\n",
    "\n",
    "    # jobCard_jobCard_lists_item__YxRkV jobCard_jobIcon__3FB1t\n",
    "#tml body div#__next div.main div.container.jsrpcomponent_mobile_container__boS9b.jsrpFixBody div.jsrpcomponent_jsrp__DUsWy.mt-15.jsrp div.jsrpcomponent_jsrp_leftPanel__2nm3w.jsrp_leftPanel div.leftpanel_jsrp_leftPanel_container__AVUCY.custom_scroll div.leftpanel_jsrp_leftPanel_wrap__YXu_c.jsrp_leftPanel_wrap div#1.parentClass.position-relative div div.jobCard_jobCard__jjUmu.active.white-box-border.jobCard div div.jobCard_jobCard_lists__fdnsc div.jobCard_jobCard_lists_item__YxRkV.jobCard_jobIcon__3FB1t    \n",
    "exprienc_tags = driver.find_elements(By.CLASS_NAME, 'jobCard_jobIcon__3FB1t')\n",
    "for i in exprienc_tags:\n",
    "    exprience.append(i.text)\n",
    "    \n",
    "data = {'Title': titles[:10], 'Employer': employers[:10], 'Expriennce': exprience[:10]}\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e599d7",
   "metadata": {},
   "source": [
    "Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "6. Brand\n",
    "7. Product Description\n",
    "8. Price\n",
    "The attributes which you have to scrape is ticked marked in the below image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "b60bb7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Type                                              Brand   Price  \\\n",
      "0    VINCENT CHASE  by Lenskart Polarized, UV Protection Rectangul...    ₹829   \n",
      "1    VINCENT CHASE  by Lenskart Polarized, UV Protection Aviator S...    ₹849   \n",
      "2             SRPM             UV Protection Wayfarer Sunglasses (50)    ₹149   \n",
      "3         Fastrack             UV Protection Wayfarer Sunglasses (58)    ₹599   \n",
      "4        Elligator         UV Protection Retro Square Sunglasses (54)    ₹149   \n",
      "..             ...                                                ...     ...   \n",
      "115  VINCENT CHASE  by Lenskart Polarized, UV Protection Round Sun...    ₹829   \n",
      "116      ROYAL SON  UV Protection Wrap-around, Sports Sunglasses (60)    ₹636   \n",
      "117      ROYAL SON         UV Protection Retro Square Sunglasses (60)    ₹497   \n",
      "118      Royaltail                  Polarized Aviator Sunglasses (58)  ₹1,118   \n",
      "119  VINCENT CHASE  by Lenskart Polarized, UV Protection Aviator S...    ₹829   \n",
      "\n",
      "    Discounts  \n",
      "0     58% off  \n",
      "1     57% off  \n",
      "2     88% off  \n",
      "3     45% off  \n",
      "4     70% off  \n",
      "..        ...  \n",
      "115   58% off  \n",
      "116   68% off  \n",
      "117   75% off  \n",
      "118   77% off  \n",
      "119   58% off  \n",
      "\n",
      "[120 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "driver= webdriver.Chrome()\n",
    "driver.get(\"https://www.flipkart.com/\")\n",
    "\n",
    "def tag_data(tags, data):\n",
    "    for tag in tags:\n",
    "        types_tag = tag.find_element(By.CLASS_NAME, \"_2WkVRV\")\n",
    "        data['Type'].append(types_tag.text)\n",
    "        brand_tag = tag.find_element(By.CLASS_NAME, \"IRpwTa\")\n",
    "        data['Brand'].append(brand_tag.text)\n",
    "        price_tag = tag.find_element(By.CLASS_NAME, \"_30jeq3\")\n",
    "        data['Price'].append(price_tag.text)\n",
    "        discount_tag = tag.find_element(By.CLASS_NAME, \"_3Ay6Sb\")\n",
    "        data['Discounts'].append(discount_tag.text)\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "searchBar = driver.find_element(By.CLASS_NAME, 'Pke_EE')\n",
    "searchBar.send_keys('sunglasses')\n",
    "searchBar.submit()\n",
    "driver.implicitly_wait(5)\n",
    "types = []\n",
    "brands = []\n",
    "price = []\n",
    "discounts = []\n",
    "data = {'Type': types, 'Brand': brands, 'Price': price, 'Discounts': discounts}\n",
    "parent_tags = driver.find_elements(By.CLASS_NAME, \"_1xHGtK\")\n",
    "\n",
    "\n",
    "tag_data(parent_tags, data)\n",
    "\n",
    "next = driver.find_element(By.CLASS_NAME, \"_1LKTO3\")\n",
    "next.click()\n",
    "element = WebDriverWait(driver, 60).until(\n",
    "        EC.presence_of_element_located((By.XPATH, \"//*[contains(text(),'Page 2 of')]\"))\n",
    "    )\n",
    "parent1_tags = driver.find_elements(By.CLASS_NAME, \"_1xHGtK\")\n",
    "tag_data(parent1_tags, data)\n",
    "\n",
    "next = driver.find_elements(By.CLASS_NAME, \"_1LKTO3\")\n",
    "next[1].click()\n",
    "element = WebDriverWait(driver, 60).until(\n",
    "        EC.presence_of_element_located((By.XPATH, \"//*[contains(text(),'Page 3 of')]\"))\n",
    "    )\n",
    "parent1_tags = driver.find_elements(By.CLASS_NAME, \"_1xHGtK\")\n",
    "tag_data(parent1_tags, data)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301d0225",
   "metadata": {},
   "source": [
    "5.Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link:\n",
    "https://www.flipkart.com/apple-iphone-11-black-64-gb/product-\n",
    "reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&market\n",
    "place=FLIPKAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "0d9a0b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Rating                Lable  \\\n",
      "0      5    Worth every penny   \n",
      "1      5            Just wow!   \n",
      "2      5  Best in the market!   \n",
      "3      5            Wonderful   \n",
      "4      5       Classy product   \n",
      "5      5             Terrific   \n",
      "6      5    Terrific purchase   \n",
      "7      5     Perfect product!   \n",
      "8      5     Perfect product!   \n",
      "9      5            Fabulous!   \n",
      "\n",
      "                                              Detail  \n",
      "0  Feeling awesome after getting the delivery of ...  \n",
      "1                                  Perfect Product!!  \n",
      "2                                        Good Camera  \n",
      "3                             This is amazing at all  \n",
      "4  Camera is awesome\\nBest battery backup\\nA perf...  \n",
      "5                                     Very very good  \n",
      "6                                  Value for money 😍  \n",
      "7                                       Photos super  \n",
      "8                                         V Good all  \n",
      "9                    Super🔥 and good performance 👌❤️  \n"
     ]
    }
   ],
   "source": [
    "driver= webdriver.Chrome()\n",
    "driver.get(\"https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART\")\n",
    "\n",
    "def tag_data(tags, data):\n",
    "    for tag in tags:\n",
    "        types_tag = tag.find_element(By.CLASS_NAME, \"_1BLPMq\")\n",
    "        data['Rating'].append(types_tag.text)\n",
    "        brand_tag = tag.find_element(By.CLASS_NAME, \"_2-N8zT\")\n",
    "        data['Lable'].append(brand_tag.text)\n",
    "        price_tag = tag.find_element(By.CLASS_NAME, \"t-ZTKy\")\n",
    "        data['Detail'].append(price_tag.text)\n",
    "    \n",
    "\n",
    "driver.implicitly_wait(5)\n",
    "ratings = []\n",
    "lables = []\n",
    "details = []\n",
    "data = {'Rating': ratings, 'Lable': lables, 'Detail': details}\n",
    "parent_tags = driver.find_elements(By.CLASS_NAME, \"_2wzgFH\")\n",
    "tag_data(parent_tags, data)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8538d3aa",
   "metadata": {},
   "source": [
    "Q6: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the\n",
    "search field.\n",
    "You have to scrape 3 attributes of each sneaker:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "As shown in the below image, you have to scrape the above attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd43cda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Type                                              Brand  \\\n",
      "0            URBANBOX  Trending Stylish Casual Outdoor Sneakers Shoes...   \n",
      "1                aadi                                   Sneakers For Men   \n",
      "2            URBANBOX  Trending Stylish Casual Outdoor Sneakers Shoes...   \n",
      "3              BRUTON               Modern Trendy Shoes Sneakers For Men   \n",
      "4           Deals4you                                 Sneakers For Women   \n",
      "..                ...                                                ...   \n",
      "115  ADIDAS ORIGINALS                        CAMPUS 00s Sneakers For Men   \n",
      "116            KILLER  KL20092 Lightweight Comfort Summer Trendy Prem...   \n",
      "117              ATOM                                   Sneakers For Men   \n",
      "118          Skechers                        GLIDE-STEP Sneakers For Men   \n",
      "119             asian  Skypy-31 Walking Shoes,Training Shoes,Sneakers...   \n",
      "\n",
      "      Price Discounts  \n",
      "0      ₹299   70% off  \n",
      "1      ₹299   85% off  \n",
      "2      ₹299   70% off  \n",
      "3      ₹299   76% off  \n",
      "4      ₹399   60% off  \n",
      "..      ...       ...  \n",
      "115  ₹6,058   39% off  \n",
      "116    ₹683   79% off  \n",
      "117  ₹1,088   66% off  \n",
      "118  ₹3,369   55% off  \n",
      "119    ₹689   13% off  \n",
      "\n",
      "[120 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "driver= webdriver.Chrome()\n",
    "driver.get(\"https://www.flipkart.com/\")\n",
    "\n",
    "def tag_data(tags, data):\n",
    "    for tag in tags:\n",
    "        types_tag = tag.find_element(By.CLASS_NAME, \"_2WkVRV\")\n",
    "        data['Type'].append(types_tag.text)\n",
    "        brand_tag = tag.find_element(By.CLASS_NAME, \"IRpwTa\")\n",
    "        data['Brand'].append(brand_tag.text)\n",
    "        price_tag = tag.find_element(By.CLASS_NAME, \"_30jeq3\")\n",
    "        data['Price'].append(price_tag.text)\n",
    "        discount_tag = tag.find_element(By.CLASS_NAME, \"_3Ay6Sb\")\n",
    "        data['Discounts'].append(discount_tag.text)\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "searchBar = driver.find_element(By.CLASS_NAME, 'Pke_EE')\n",
    "searchBar.send_keys('sneakers')\n",
    "searchBar.submit()\n",
    "driver.implicitly_wait(5)\n",
    "types = []\n",
    "brands = []\n",
    "price = []\n",
    "discounts = []\n",
    "data = {'Type': types, 'Brand': brands, 'Price': price, 'Discounts': discounts}\n",
    "parent_tags = driver.find_elements(By.CLASS_NAME, \"_1xHGtK\")\n",
    "\n",
    "\n",
    "tag_data(parent_tags, data)\n",
    "\n",
    "next = driver.find_element(By.CLASS_NAME, \"_1LKTO3\")\n",
    "next.click()\n",
    "element = WebDriverWait(driver, 60).until(\n",
    "        EC.presence_of_element_located((By.XPATH, \"//*[contains(text(),'Page 2 of')]\"))\n",
    "    )\n",
    "parent1_tags = driver.find_elements(By.CLASS_NAME, \"_1xHGtK\")\n",
    "tag_data(parent1_tags, data)\n",
    "\n",
    "next = driver.find_elements(By.CLASS_NAME, \"_1LKTO3\")\n",
    "next[1].click()\n",
    "element = WebDriverWait(driver, 60).until(\n",
    "        EC.presence_of_element_located((By.XPATH, \"//*[contains(text(),'Page 3 of')]\"))\n",
    "    )\n",
    "parent1_tags = driver.find_elements(By.CLASS_NAME, \"_1xHGtK\")\n",
    "tag_data(parent1_tags, data)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ae31e8",
   "metadata": {},
   "source": [
    "Q7: Go to webpage https://www.amazon.in/ Enter “Laptop” in the search field and then click the search icon. Then\n",
    "set CPU Type filter to “Intel Core i7” as shown in the below image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "90e95a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Product     Price\n",
      "0   Sponsored\\nMicrosoft\\nSurface Laptop GO 3 Touc...    78,999\n",
      "1   Sponsored\\nDell\\n15 Laptop, Intel Core i5-1135...    51,990\n",
      "2   HP\\nPavilion Aero AMD Ryzen 7 7735U 13.3 inch(...    77,990\n",
      "3   ASUS\\nVivoBook 15 (2021), 15.6-inch (39.62 cm)...    22,990\n",
      "4   HP\\nLaptop 15s, Intel Celeron, 15.6-inch (39.6...    29,490\n",
      "5   Dell G15 5520 Gaming Laptop, Intel i5-12500H, ...    71,490\n",
      "6   ASUS Vivobook S15 OLED 2022, 15.6\" 39.62 cm FH...    74,990\n",
      "7   Lenovo Yoga Slim 6 Intel Evo Core i5 1240P 14\"...    64,519\n",
      "8                                                              \n",
      "9                                                              \n",
      "10  Sponsored\\nASUS ROG Strix G17 (2022), 17.3-inc...    88,990\n",
      "11  [Windows 11 Home] HP 255 G8 Notebook, 15.6'' H...    22,680\n",
      "12  Dell\\n15 Laptop, Intel Core i3-1115G4 Processo...    37,490\n",
      "13  HP\\n15s 12th Gen Intel Core i5-1235U 15/6inch ...    51,990\n",
      "14  ASUS\\nVivobook 14, Intel Core i3-1115G4 11th G...    29,990\n",
      "15  HP\\nPavilion Aero AMD Ryzen 5 7535U 13.3 inch(...    68,999\n",
      "16  Sponsored\\nApple\\n2023 MacBook Air Laptop with...  1,54,900\n",
      "17  Sponsored\\nMicrosoft\\nNew Surface Pro9 13\" Int...  1,02,520\n",
      "18  Tecno\\nMEGABOOK T1, Intel Core 11th Gen i3 Pro...    23,990\n",
      "19  Lenovo\\nIdeaPad Slim 5 13th Gen Intel Core i7-...    83,190\n",
      "20  Tecno\\nMEGABOOK T1, Intel Core 11th Gen i5 Pro...    29,990\n",
      "21  HP\\nLaptop 15s, 12th Gen Intel Core i3-1215U, ...    38,999\n",
      "22  Dell\\n15 Laptop, Intel Core i5-1135G7 Processo...    47,990\n",
      "23  Tecno\\nMEGABOOK T1, Intel Core 11th Gen i5 Pro...    29,990\n",
      "24  Acer\\nTravelmate Business Laptop AMD Ryzen 5 5...    34,990\n",
      "25  ASUS\\nSmartChoice] Vivobook 15, Intel Celeron ...    30,990\n",
      "26  Sponsored\\nYale\\nStandard Safe For Laptop YLS/...     7,000\n",
      "27  Sponsored\\nDell\\n15 Laptop, Intel Core i3-1115...    37,490\n",
      "28  Sponsored\\nDell G15 5520 Gaming Laptop, Intel ...    77,490\n"
     ]
    }
   ],
   "source": [
    "driver= webdriver.Chrome()\n",
    "driver.get(\"https://www.amazon.in/\")\n",
    "search_g= driver.find_element(By.XPATH, \"//input[@type='text']\")\n",
    "search_g.send_keys('Laptop')\n",
    "search_btn=driver.find_element(By.XPATH, \"//input[@id='nav-search-submit-button']\")\n",
    "search_btn.click()\n",
    "\n",
    "Title=[]\n",
    "Price=[]\n",
    "Rating=[]\n",
    "WebDriverWait(driver, 60).until(\n",
    "        EC.presence_of_element_located((By.XPATH, \"//div[@data-cy='title-recipe']\"))\n",
    "    )\n",
    "b_name=driver.find_elements(By.XPATH, \"//div[@data-cy='title-recipe']\")\n",
    "price =driver.find_elements(By.XPATH, \"//span[@class='a-price-whole']\")\n",
    "\n",
    "\n",
    "for j  in b_name:\n",
    "    Title.append(j.text)\n",
    "   \n",
    " for l in price:\n",
    "    Price.append(l.text)\n",
    "\n",
    "    \n",
    "df = pd.DataFrame({'Product': Title, 'Price': Price})\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8a5105",
   "metadata": {},
   "source": [
    "Q8: Write a python program to scrape data for Top 1000 Quotes of All Time.\n",
    "The above task will be done in following steps:\n",
    "1. First get the webpagehttps://www.azquotes.com/\n",
    "2. Click on Top Quotes\n",
    "3. Than scrap a) Quote b) Author c) Type Of Quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "f363931c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Quote                Author  \\\n",
      "0   The essence of strategy is choosing what not t...        Michael Porter   \n",
      "1   One cannot and must not try to erase the past ...            Golda Meir   \n",
      "2   Patriotism means to stand by the country. It d...    Theodore Roosevelt   \n",
      "3   Death is something inevitable. When a man has ...        Nelson Mandela   \n",
      "4   You have to love a nation that celebrates its ...          Erma Bombeck   \n",
      "..                                                ...                   ...   \n",
      "95     When the going gets weird, the weird turn pro.    Hunter S. Thompson   \n",
      "96  When a train goes through a tunnel and it gets...       Corrie Ten Boom   \n",
      "97  If you think you are too small to make a diffe...            Dalai Lama   \n",
      "98  God doesn't require us to succeed, he only req...         Mother Teresa   \n",
      "99    Change your thoughts and you change your world.  Norman Vincent Peale   \n",
      "\n",
      "                                        Type  \n",
      "0   Essence, Deep Thought, Transcendentalism  \n",
      "1                  Inspiration, Past, Trying  \n",
      "2                        Country, Peace, War  \n",
      "3         Inspirational, Motivational, Death  \n",
      "4               4th Of July, Food, Patriotic  \n",
      "..                                       ...  \n",
      "95                    Music, Sports, Hunting  \n",
      "96             Trust, Encouraging, Uplifting  \n",
      "97              Inspirational, Funny, Change  \n",
      "98                      Success, God, Mother  \n",
      "99       Inspirational, Motivational, Change  \n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "driver= webdriver.Chrome()\n",
    "driver.get(\"https://www.azquotes.com/\")\n",
    "driver.find_element(By.XPATH, \"//a[contains(text(),'Top Quotes')]\").click()\n",
    "\n",
    "def tag_data(tags, data):\n",
    "    for tag in tags:\n",
    "        types_tag = tag.find_element(By.CLASS_NAME, \"title\")\n",
    "        data['Quote'].append(types_tag.text)\n",
    "        brand_tag = tag.find_element(By.CLASS_NAME, \"author\")\n",
    "        data['Author'].append(brand_tag.text)\n",
    "        price_tag = tag.find_element(By.CLASS_NAME, \"tags\")\n",
    "        data['Type'].append(price_tag.text)\n",
    "\n",
    "quotes = []\n",
    "authors = []\n",
    "types = []\n",
    "data = {'Quote': quotes, 'Author': authors, 'Type': types}\n",
    "parent_tag = driver.find_elements(By.CLASS_NAME, \"wrap-block\")\n",
    "tag_data(parent_tag, data)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cbed37",
   "metadata": {},
   "source": [
    "Q9: Write a python program to display list of respected former Prime Ministers of India(i.e. Name, Born-Dead,\n",
    "Term of office, Remarks) from https://www.jagranjosh.com/.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpagehttps://www.jagranjosh.com/\n",
    "2. Then You have to click on the GK option\n",
    "3. Then click on the List of all Prime Ministers of India\n",
    "4. Then scrap the mentioned data and make theDataFra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e94eef18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:\n",
      "                           Name     Born Date  \\\n",
      "0             Jawahar Lal Nehru   (1889–1964)   \n",
      "1     Gulzarilal Nanda (Acting)   (1898-1998)   \n",
      "2           Lal Bahadur Shastri   (1904–1966)   \n",
      "3   Gulzari Lal Nanda  (Acting)   (1898-1998)   \n",
      "4                 Indira Gandhi   (1917–1984)   \n",
      "5                 Morarji Desai   (1896–1995)   \n",
      "6                  Charan Singh   (1902–1987)   \n",
      "7                 Indira Gandhi   (1917–1984)   \n",
      "8                  Rajiv Gandhi   (1944–1991)   \n",
      "9                   V. P. Singh   (1931–2008)   \n",
      "10              Chandra Shekhar   (1927–2007)   \n",
      "11          P. V. Narasimha Rao   (1921–2004)   \n",
      "12         Atal Bihari Vajpayee  (1924- 2018)   \n",
      "13             H. D. Deve Gowda   (born 1933)   \n",
      "14           Inder Kumar Gujral   (1919–2012)   \n",
      "15         Atal Bihari Vajpayee   (1924-2018)   \n",
      "16               Manmohan Singh   (born 1932)   \n",
      "17                Narendra Modi   (born 1950)   \n",
      "18                Narendra Modi   (born 1950)   \n",
      "\n",
      "                                       Term In Office  \n",
      "0   15 August 1947 to 27 May 1964\\n16 years, 286 days  \n",
      "1                27 May 1964 to 9 June 1964,\\n13 days  \n",
      "2    9 June 1964 to 11 January 1966\\n1 year, 216 days  \n",
      "3         11 January 1966 to 24 January 1966\\n13 days  \n",
      "4   24 January 1966 to 24 March 1977\\n11 years, 59...  \n",
      "5   24 March 1977 to  28 July 1979 \\n2 year, 126 days  \n",
      "6           28 July 1979 to 14 January 1980\\n170 days  \n",
      "7   14 January 1980 to 31 October 1984\\n4 years, 2...  \n",
      "8   31 October 1984 to 2 December 1989\\n5 years, 3...  \n",
      "9       2 December 1989 to 10 November 1990\\n343 days  \n",
      "10         10 November 1990 to 21 June 1991\\n223 days  \n",
      "11     21 June 1991 to 16 May 1996\\n4 years, 330 days  \n",
      "12                16 May 1996 to 1 June 1996\\n16 days  \n",
      "13             1 June 1996 to 21 April 1997\\n324 days  \n",
      "14          21 April 1997 to 19 March 1998 \\n332 days  \n",
      "15    19 March 1998 to 22 May 2004 \\n6 years, 64 days  \n",
      "16    22 May 2004 to 26 May 2014   \\n10 years, 4 days  \n",
      "17                                 26 May 2014 - 2019  \n",
      "18                             30 May 2019- Incumbent  \n"
     ]
    }
   ],
   "source": [
    "driver= webdriver.Chrome()\n",
    "driver.get(\"https://www.jagranjosh.com/\")\n",
    "driver.find_element(By.XPATH, \"//a[contains(text(),'GK')]\").click()\n",
    "\n",
    "element = WebDriverWait(driver, 60).until(\n",
    "        EC.presence_of_element_located((By.LINK_TEXT, \"List of All Prime Ministers of India (1947-2024)\"))\n",
    "    )\n",
    "pm_option = driver.find_element(By.LINK_TEXT, \"List of All Prime Ministers of India (1947-2024)\")\n",
    "pm_option.click()\n",
    "\n",
    "element = WebDriverWait(driver, 5).until(\n",
    "        EC.presence_of_element_located((By.ID, 'itemdiv'))\n",
    "    )\n",
    "\n",
    "\n",
    "data = []\n",
    "table = driver.find_element(By.ID, 'itemdiv')\n",
    "rows = table.find_elements(By.TAG_NAME , 'tr')\n",
    "for row in rows:\n",
    "    tds = row.find_elements(By.TAG_NAME , 'td')\n",
    "    if len(tds) > 4:\n",
    "        data.append({\"Name\": tds[1].text, \"Born Date\": tds[2].text , \"Term In Office\": tds[3].text })\n",
    "    \n",
    "print('data:')\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n",
    "#List of All Prime Ministers of India (1947-2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79005bc",
   "metadata": {},
   "source": [
    "Q10: Write a python program to display list of 50 Most expensive cars in the world (i.e.\n",
    "Car name and Price) from https://www.motor1.com/\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.motor1.com/\n",
    "2. Then You have to type in the search bar ’50 most expensive cars’\n",
    "3. Then click on 50 most expensive cars in the world..\n",
    "4. Then scrap the mentioned data and make the datafram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "151b8a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Car           Price\n",
      "0               Aston Martin Valour     $1.5 Million\n",
      "1                      Mclaren Elva     $1.7 Million\n",
      "2                       Czinger 21C     $1.7 Million\n",
      "3                     Ferrari Monza     $1.7 Million\n",
      "4                Gordon Murray T.33     $1.7 Million\n",
      "5                 Koenigsegg Gemera     $1.7 Million\n",
      "6                      McLaren Elva     $1.7 Million\n",
      "7                Hennessey Venom F5     $1.8 Million\n",
      "8                   Bentley Bacalar     $1.9 Million\n",
      "9                       SSC Tuatara     $2.0 Million\n",
      "10                      Lotus Evija     $2.1 Million\n",
      "11              Aston Martin Vulcan     $2.3 Million\n",
      "12                       Delage D12     $2.3 Million\n",
      "13              Ferrari Daytona SP3     $2.3 Million\n",
      "14                McLaren Speedtail     $2.3 Million\n",
      "15                     Rimac Nevera     $2.4 Million\n",
      "16             Pininfarina Battista     $2.5 Million\n",
      "17               Gordon Murray T.50     $2.6 Million\n",
      "18             Lamborghini Countach     $2.6 Million\n",
      "19                     Zenvo Aurora     $2.8 Million\n",
      "20              Aston Martin Victor     $3.0 Million\n",
      "21                 Koenigsegg Jesko     $3.0 Million\n",
      "22            Aston Martin Valkyrie     $3.2 Million\n",
      "23        W Motors Lykan Hypersport     $3.4 Million\n",
      "24                 Lamborghini Sian     $3.6 million\n",
      "25  Bugatti Chiron Super Sport 300+     $3.9 Million\n",
      "26               Lamborghini Veneno     $4.5 Million\n",
      "27                   Bugatti Bolide     $4.7 Million\n",
      "28        Pininfarina B95 Speedster     $4.8 Million\n",
      "29                     Bugatti Divo     $5.8 Million\n",
      "30              SP Automotive Chaos     $6.4 Million\n",
      "31                     777 Hypercar     $7.5 Million\n",
      "32               Bugatti Centodieci     $9.0 Million\n",
      "33         Bugatti La Voiture Noire    $13.4 Million\n"
     ]
    }
   ],
   "source": [
    "driver= webdriver.Chrome()\n",
    "driver.get(\"https://www.motor1.com/\")\n",
    "driver.find_element(By.CLASS_NAME, \"m1-search-panel-input\").send_keys(\"50 most expensive cars\")\n",
    "driver.find_element(By.CLASS_NAME, \"m1-search-panel-button\").click()\n",
    "element = WebDriverWait(driver, 60).until(\n",
    "        EC.presence_of_element_located((By.LINK_TEXT, \"50 Most Expensive Cars In The World\"))\n",
    "    )\n",
    "pm_option = driver.find_element(By.LINK_TEXT, \"50 Most Expensive Cars In The World\")\n",
    "pm_option.click()\n",
    "element = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.XPATH, \"//h1[contains(text(),'50 Most Expensive Cars In The World')]\"))\n",
    "    )\n",
    "data = driver.find_element(By.CLASS_NAME, \"postBody\")\n",
    "vehicles = data.find_elements(By.TAG_NAME, \"li\")\n",
    "info = []\n",
    "for item in vehicles:\n",
    "    itemInfo = item.text.split('-')\n",
    "    if (len(itemInfo) == 2):\n",
    "        info.append({'Car': itemInfo[0], 'Price': itemInfo[1]})\n",
    "    \n",
    "df = pd.DataFrame(info)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca59e16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c74f337",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
